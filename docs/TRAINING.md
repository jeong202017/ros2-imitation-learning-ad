# ğŸ“ Training Guide / í•™ìŠµ ê°€ì´ë“œ

This guide explains how to train imitation learning models for the ERP42 vehicle.

ì´ ê°€ì´ë“œëŠ” ERP42 ì°¨ëŸ‰ì„ ìœ„í•œ ëª¨ë°©í•™ìŠµ ëª¨ë¸ í•™ìŠµ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## Overview / ê°œìš”

The training process involves:
1. Loading preprocessed data
2. Configuring model and training parameters
3. Training with automatic checkpointing
4. Monitoring with TensorBoard
5. Evaluating the trained model

í•™ìŠµ ê³¼ì •ì€ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
2. ëª¨ë¸ ë° í•™ìŠµ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
3. ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ê³¼ í•¨ê»˜ í•™ìŠµ
4. TensorBoardë¡œ ëª¨ë‹ˆí„°ë§
5. í•™ìŠµëœ ëª¨ë¸ í‰ê°€

## Hyperparameter Configuration / í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

### Experiment 1: Camera-Only / ì‹¤í—˜ 1: ì¹´ë©”ë¼ ì „ìš©

Configuration file: `configs/experiment1_camera_only.yaml`

```yaml
experiment:
  name: "experiment1_camera_only"
  description: "Baseline using front camera only"

data:
  camera_topic: /video1
  control_topic: /Control/serial_data
  control_indices:
    speed: 3
    steering: 4
  image_size: [224, 224]
  use_lidar: false

model:
  type: cnn
  backbone: resnet18        # or resnet34, resnet50
  pretrained: true          # Use ImageNet pretrained weights
  output_dim: 2             # [speed, steering]

training:
  batch_size: 32            # Reduce if OOM
  epochs: 100
  learning_rate: 0.001      # Try 0.0001 for fine-tuning
  optimizer: adam           # or sgd, adamw
  weight_decay: 0.0001
  lr_scheduler: step        # step, cosine, or none
  lr_step_size: 30          # For step scheduler
  lr_gamma: 0.1             # Multiply LR by this every step
  early_stopping_patience: 15
  gradient_clip: 1.0        # Gradient clipping value

loss:
  type: mse                 # or mae, huber
  
augmentation:
  enabled: true
  horizontal_flip: 0.0      # Don't flip for driving
  brightness: 0.2
  contrast: 0.2
  rotation: 0               # No rotation for driving
  blur: 0.1

device: cuda                # or cpu
```

### Experiment 2: Multimodal / ì‹¤í—˜ 2: ë©€í‹°ëª¨ë‹¬

Configuration file: `configs/experiment2_multimodal.yaml`

```yaml
experiment:
  name: "experiment2_multimodal"
  description: "Camera + LiDAR sensor fusion"

data:
  camera_topic: /video1
  lidar_pointcloud: /velodyne_points_filtered
  lidar_scan: /scan
  control_topic: /Control/serial_data
  use_lidar: true
  use_pointcloud: true      # Use 3D point cloud
  use_laserscan: true       # Use 2D laser scan

model:
  type: multimodal
  camera_encoder:
    backbone: resnet18
    pretrained: true
    output_dim: 256
  lidar_encoder:
    type: pointnet_simple   # or cnn1d for laser scan
    output_dim: 256
    hidden_dims: [512, 256]
  fusion:
    type: concatenate       # or attention, bilinear
    hidden_dim: 512
    dropout: 0.3
  output_dim: 2

training:
  batch_size: 16            # Smaller due to point clouds
  epochs: 100
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.0001
  lr_scheduler: step
  lr_step_size: 30
  lr_gamma: 0.1
  early_stopping_patience: 15
  gradient_clip: 1.0

loss:
  type: mse
  
device: cuda
```

## Training Scripts / í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

### Camera-Only Training / ì¹´ë©”ë¼ ì „ìš© í•™ìŠµ

```bash
python scripts/train_camera_only.py \
    --config configs/experiment1_camera_only.yaml \
    --data-dir data/processed/experiment1 \
    --output-dir models/experiment1 \
    --epochs 100 \
    --device cuda
```

**Parameters**:
- `--config`: Path to configuration file
- `--data-dir`: Directory with preprocessed data
- `--output-dir`: Where to save models and logs
- `--epochs`: Number of training epochs (overrides config)
- `--device`: 'cuda' or 'cpu'
- `--resume`: Path to checkpoint to resume training

### Multimodal Training / ë©€í‹°ëª¨ë‹¬ í•™ìŠµ

```bash
python scripts/train_multimodal.py \
    --config configs/experiment2_multimodal.yaml \
    --data-dir data/processed/experiment2 \
    --output-dir models/experiment2 \
    --epochs 100 \
    --device cuda
```

## Training Monitoring / í•™ìŠµ ëª¨ë‹ˆí„°ë§

### TensorBoard Usage / TensorBoard ì‚¬ìš©ë²•

Start TensorBoard to monitor training in real-time:

ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµì„ ëª¨ë‹ˆí„°ë§í•˜ë ¤ë©´ TensorBoardë¥¼ ì‹œì‘í•˜ì„¸ìš”:

```bash
tensorboard --logdir models/experiment1/logs --port 6006
```

Then open your browser at: `http://localhost:6006`

ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ìŒ ì£¼ì†Œë¥¼ ì—¬ì„¸ìš”: `http://localhost:6006`

### What to Monitor / ëª¨ë‹ˆí„°ë§ í•­ëª©

**Scalars / ìŠ¤ì¹¼ë¼**:
- `loss/train`: Training loss per epoch (ì—í­ë‹¹ í•™ìŠµ ì†ì‹¤)
- `loss/val`: Validation loss per epoch (ì—í­ë‹¹ ê²€ì¦ ì†ì‹¤)
- `metrics/speed_mae`: Speed prediction MAE
- `metrics/steering_mae`: Steering prediction MAE
- `learning_rate`: Current learning rate (í˜„ì¬ í•™ìŠµë¥ )

**Images / ì´ë¯¸ì§€** (if enabled):
- Sample predictions vs. ground truth
- Input images with overlays

**Histograms / íˆìŠ¤í† ê·¸ë¨**:
- Model weights distribution
- Gradient distribution

### Training Curves / í•™ìŠµ ê³¡ì„ 

**Healthy Training** / ì •ìƒ í•™ìŠµ:
- Loss decreases smoothly
- Train and val loss decrease together
- Validation loss plateaus or slightly increases at the end

**Overfitting** / ê³¼ì í•©:
- Train loss << Val loss
- Val loss increases while train loss decreases
- **Solutions**:
  - Increase dropout
  - Add data augmentation
  - Reduce model capacity
  - Early stopping

**Underfitting** / ê³¼ì†Œì í•©:
- Both train and val loss remain high
- Loss plateaus early
- **Solutions**:
  - Increase model capacity
  - Train longer
  - Reduce regularization
  - Check data quality

## Checkpoint Management / ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### Automatic Saving / ìë™ ì €ì¥

The training script automatically saves:
- `best_model.pth`: Model with lowest validation loss (ìµœì € ê²€ì¦ ì†ì‹¤ ëª¨ë¸)
- `last_model.pth`: Most recent model (ìµœì‹  ëª¨ë¸)
- `checkpoint_epoch_N.pth`: Periodic checkpoints (ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸)

### Checkpoint Contents / ì²´í¬í¬ì¸íŠ¸ ë‚´ìš©

Each checkpoint contains:
```python
{
    'epoch': 50,
    'model_state_dict': ...,
    'optimizer_state_dict': ...,
    'scheduler_state_dict': ...,
    'train_loss': 0.0234,
    'val_loss': 0.0456,
    'config': {...}
}
```

### Resume Training / í•™ìŠµ ì¬ê°œ

To resume training from a checkpoint:

ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµì„ ì¬ê°œí•˜ë ¤ë©´:

```bash
python scripts/train_camera_only.py \
    --config configs/experiment1_camera_only.yaml \
    --data-dir data/processed/experiment1 \
    --output-dir models/experiment1 \
    --resume models/experiment1/checkpoint_epoch_50.pth
```

## Training Tips / í•™ìŠµ íŒ

### Learning Rate Selection / í•™ìŠµë¥  ì„ íƒ

**Too High** (ë„ˆë¬´ ë†’ìŒ):
- Loss oscillates or diverges
- Training unstable

**Too Low** (ë„ˆë¬´ ë‚®ìŒ):
- Very slow convergence
- Gets stuck in local minima

**Good Starting Points** (ì¢‹ì€ ì‹œì‘ì ):
- Pretrained model: `1e-4` to `1e-3`
- From scratch: `1e-3` to `1e-2`

### Batch Size Effects / ë°°ì¹˜ í¬ê¸° íš¨ê³¼

**Larger Batch Size** (í° ë°°ì¹˜ í¬ê¸°):
- More stable gradients
- Faster training (better GPU utilization)
- Requires more memory
- May need higher learning rate

**Smaller Batch Size** (ì‘ì€ ë°°ì¹˜ í¬ê¸°):
- More noise in gradients (can help escape local minima)
- Less memory usage
- Slower training
- May need lower learning rate

### Data Augmentation / ë°ì´í„° ì¦ê°•

**Recommended for Driving** (ì£¼í–‰ì— ê¶Œì¥):
- âœ… Brightness/contrast adjustment
- âœ… Blur (simulates motion blur)
- âŒ Horizontal flip (changes driving direction)
- âŒ Rotation (unrealistic for driving)
- âŒ Vertical flip (never happens in driving)

### Transfer Learning / ì „ì´ í•™ìŠµ

**Benefits** (ì´ì ):
- Faster convergence
- Better performance with limited data
- Pretrained on ImageNet (natural images)

**Usage**:
- Set `pretrained: true` in config
- Use lower learning rate (e.g., 1e-4)
- Optionally freeze early layers initially

## Common Issues / ì¼ë°˜ì ì¸ ë¬¸ì œ

### Out of Memory (OOM) / ë©”ëª¨ë¦¬ ë¶€ì¡±

**Symptoms** (ì¦ìƒ):
- CUDA out of memory error
- Training crashes

**Solutions** (í•´ê²°ì±…):
1. Reduce batch size
2. Use smaller images (e.g., 128x128 instead of 224x224)
3. Use gradient accumulation
4. Use mixed precision training (FP16)

```yaml
training:
  batch_size: 16  # Reduce from 32
  gradient_accumulation_steps: 2  # Effective batch size = 32
```

### Loss Not Decreasing / ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•ŠìŒ

**Check**:
1. Data loading is correct
2. Data normalization is applied
3. Learning rate is not too low
4. Model architecture matches data

**Solutions**:
1. Visualize input data
2. Check label distribution
3. Try different learning rate
4. Simplify model first

### NaN Loss / NaN ì†ì‹¤

**Causes** (ì›ì¸):
- Learning rate too high
- Numerical instability
- Invalid data (inf, nan values)

**Solutions** (í•´ê²°ì±…):
1. Reduce learning rate
2. Enable gradient clipping
3. Check data preprocessing
4. Use mixed precision carefully

## Performance Optimization / ì„±ëŠ¥ ìµœì í™”

### Speed Up Training / í•™ìŠµ ê°€ì†í™”

1. **Use GPU** (GPU ì‚¬ìš©):
   ```yaml
   device: cuda
   ```

2. **Increase Batch Size** (ë°°ì¹˜ í¬ê¸° ì¦ê°€):
   - Up to GPU memory limit

3. **Use More Workers** (ë” ë§ì€ ì›Œì»¤ ì‚¬ìš©):
   ```python
   DataLoader(..., num_workers=4)
   ```

4. **Pin Memory** (ë©”ëª¨ë¦¬ ê³ ì •):
   ```python
   DataLoader(..., pin_memory=True)
   ```

5. **Mixed Precision Training** (í˜¼í•© ì •ë°€ë„ í•™ìŠµ):
   ```python
   from torch.cuda.amp import autocast, GradScaler
   ```

### Reduce Memory Usage / ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œ

1. **Gradient Accumulation** (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì )
2. **Checkpoint Gradients** (ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸)
3. **Smaller Batch Size** (ì‘ì€ ë°°ì¹˜ í¬ê¸°)
4. **Reduce Image Resolution** (ì´ë¯¸ì§€ í•´ìƒë„ ê°ì†Œ)

## Model Evaluation / ëª¨ë¸ í‰ê°€

After training, evaluate the model:

í•™ìŠµ í›„ ëª¨ë¸ì„ í‰ê°€í•˜ì„¸ìš”:

```bash
python scripts/evaluate_model.py \
    --config configs/experiment1_camera_only.yaml \
    --data-dir data/processed/experiment1 \
    --model-path models/experiment1/best_model.pth \
    --output-dir results/experiment1
```

This generates:
- Metrics JSON file (ì§€í‘œ JSON íŒŒì¼)
- Prediction vs. ground truth plots (ì˜ˆì¸¡ ëŒ€ ì‹¤ì œ í”Œë¡¯)
- Error distribution histograms (ì˜¤ë¥˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨)

## Next Steps / ë‹¤ìŒ ë‹¨ê³„

1. **Compare Experiments**: Use `compare_experiments.py` to compare results
2. **Hyperparameter Tuning**: Experiment with different settings
3. **Ensemble Models**: Combine multiple models for better performance
4. **Deploy**: Use the trained model in ROS2 inference node

1. **ì‹¤í—˜ ë¹„êµ**: `compare_experiments.py`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ ë¹„êµ
2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ë‹¤ì–‘í•œ ì„¤ì • ì‹¤í—˜
3. **ì•™ìƒë¸” ëª¨ë¸**: ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
4. **ë°°í¬**: í•™ìŠµëœ ëª¨ë¸ì„ ROS2 ì¶”ë¡  ë…¸ë“œì—ì„œ ì‚¬ìš©

## References / ì°¸ê³  ìë£Œ

- [PyTorch Training Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)
- [TensorBoard Guide](https://www.tensorflow.org/tensorboard/get_started)
- [Learning Rate Scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
- [Mixed Precision Training](https://pytorch.org/docs/stable/amp.html)
