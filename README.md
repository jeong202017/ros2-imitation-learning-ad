# ğŸš— ROS2 Bag-based Imitation Learning for Autonomous Driving

ROS2 bag íŒŒì¼ì—ì„œ ì„±ê³µí•œ ì£¼í–‰ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í•™ìŠµí•˜ëŠ” ëª¨ë°©í•™ìŠµ ê¸°ë°˜ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ROS2 bag íŒŒì¼ì— ê¸°ë¡ëœ ì„±ê³µì ì¸ ì£¼í–‰ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ììœ¨ì£¼í–‰ì„ ìˆ˜í–‰í•˜ëŠ” Behavioral Cloning ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **ROS2 Bag ë°ì´í„° ì¶”ì¶œ**: bag íŒŒì¼ì—ì„œ ì„¼ì„œ ë°ì´í„° ë° ì œì–´ ëª…ë ¹ ì¶”ì¶œ
- **Behavioral Cloning**: ì„±ê³µí•œ ì£¼í–‰ ê²½í—˜ ê¸°ë°˜ ì§€ë„í•™ìŠµ
- **ë‹¤ì¤‘ ì„¼ì„œ ìœµí•©**: ì¹´ë©”ë¼, LiDAR, IMU ë“± ë‹¤ì–‘í•œ ì„¼ì„œ ë°ì´í„° í™œìš©
- **ì‹¤ì‹œê°„ ROS2 í†µí•©**: í•™ìŠµëœ ëª¨ë¸ì„ ROS2 ë…¸ë“œë¡œ ë°°í¬
- **ë°ì´í„° í•„í„°ë§**: ì„±ê³µ/ì‹¤íŒ¨ ì£¼í–‰ ë°ì´í„° ìë™ ë¶„ë¥˜

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜
\`\`\`bash
git clone https://github.com/jeong202017/ros2-imitation-learning-ad.git
cd ros2-imitation-learning-ad
pip install -r requirements.txt
\`\`\`

### ROS2 Bag ë°ì´í„° ì¶”ì¶œ
\`\`\`bash
python scripts/extract_rosbag.py \
    --bag-path data/rosbags/successful/drive_001.db3 \
    --output-dir data/extracted/drive_001 \
    --topics /camera/image_raw /cmd_vel /odom
\`\`\`

### ëª¨ë¸ í•™ìŠµ
\`\`\`bash
python scripts/train_model.py \
    --config configs/training_config.yaml \
    --data-dir data/processed \
    --epochs 100
\`\`\`

### ROS2 ë…¸ë“œ ì‹¤í–‰
\`\`\`bash
ros2 run ros2_imitation_learning inference_node \
    --model-path models/bc_cnn_best.pth
\`\`\`

## ğŸ§ª Experiments / ì‹¤í—˜

ì´ í”„ë¡œì íŠ¸ëŠ” ERP42 ì°¨ëŸ‰ì˜ ëª¨ë°©í•™ìŠµì„ ìœ„í•œ ë‘ ê°€ì§€ ì£¼ìš” ì‹¤í—˜ì„ ì œê³µí•©ë‹ˆë‹¤:

### Experiment 1: Camera-Only Baseline / ì‹¤í—˜ 1: ì¹´ë©”ë¼ ì „ìš© ë² ì´ìŠ¤ë¼ì¸
- **Input**: Front camera images only (ì „ë°© ì¹´ë©”ë¼ ì´ë¯¸ì§€ë§Œ)
- **Model**: ResNet-18 CNN encoder
- **Purpose**: Establish baseline performance (ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì„¤ì •)

### Experiment 2: Multimodal Sensor Fusion / ì‹¤í—˜ 2: ë©€í‹°ëª¨ë‹¬ ì„¼ì„œ ìœµí•©
- **Input**: Camera + LiDAR (PointCloud & LaserScan)
- **Model**: CNN + PointNet + 1D CNN fusion
- **Purpose**: Improve spatial awareness and robustness (ê³µê°„ ì¸ì‹ ë° ê°•ê±´ì„± í–¥ìƒ)

### Quick Start / ë¹ ë¥¸ ì‹œì‘

```bash
# Run all experiments automatically
# ëª¨ë“  ì‹¤í—˜ ìë™ ì‹¤í–‰
./scripts/run_experiment.sh

# Or run individually
# ë˜ëŠ” ê°œë³„ ì‹¤í–‰

# Experiment 1: Camera-only
python scripts/extract_erp42_multiview.py --bag-path data/rosbags/001_0.db3 --output-dir data/extracted/erp42_exp1 --topics /video1 /Control/serial_data
python scripts/preprocess_camera_only.py --input-dir data/extracted/erp42_exp1 --output-dir data/processed/experiment1
python scripts/train_camera_only.py --config configs/experiment1_camera_only.yaml --data-dir data/processed/experiment1 --output-dir models/experiment1

# Experiment 2: Multimodal
python scripts/extract_erp42_multiview.py --bag-path data/rosbags/001_0.db3 --output-dir data/extracted/erp42_exp2 --topics /video1 /velodyne_points_filtered /scan /Control/serial_data
python scripts/preprocess_multimodal.py --input-dir data/extracted/erp42_exp2 --output-dir data/processed/experiment2
python scripts/train_multimodal.py --config configs/experiment2_multimodal.yaml --data-dir data/processed/experiment2 --output-dir models/experiment2

# Compare results
# ê²°ê³¼ ë¹„êµ
python scripts/compare_experiments.py --exp1-dir models/experiment1 --exp2-dir models/experiment2 --output-dir results/comparison
```

### Documentation / ë¬¸ì„œ
- ğŸ“– [Experiments Guide](docs/EXPERIMENTS.md) - Detailed experiment comparison / ìƒì„¸ ì‹¤í—˜ ë¹„êµ
- ğŸ“¦ [Data Extraction Guide](docs/DATA_EXTRACTION.md) - ERP42 bag file processing / ERP42 bag íŒŒì¼ ì²˜ë¦¬
- ğŸ“ [Training Guide](docs/TRAINING.md) - Training tips and troubleshooting / í•™ìŠµ íŒ ë° ë¬¸ì œ í•´ê²°

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
ros2-imitation-learning-ad/
â”œâ”€â”€ data/                      # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ rosbags/              # ROS2 bag íŒŒì¼
â”‚   â”œâ”€â”€ extracted/            # ì¶”ì¶œëœ ì„¼ì„œ ë°ì´í„°
â”‚   â””â”€â”€ processed/            # ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°
â”œâ”€â”€ models/                    # í•™ìŠµëœ ëª¨ë¸
â”‚   â”œâ”€â”€ experiment1/          # ì¹´ë©”ë¼ ì „ìš© ëª¨ë¸
â”‚   â””â”€â”€ experiment2/          # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
â”œâ”€â”€ results/                   # ì‹¤í—˜ ê²°ê³¼
â”‚   â””â”€â”€ comparison/           # ì‹¤í—˜ ë¹„êµ ê²°ê³¼
â”œâ”€â”€ src/                       # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ data_processing/       # ë°ì´í„° ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ dataset_camera.py       # ì¹´ë©”ë¼ ì „ìš© Dataset
â”‚   â”‚   â””â”€â”€ dataset_multimodal.py   # ë©€í‹°ëª¨ë‹¬ Dataset
â”‚   â”œâ”€â”€ models/                # ëª¨ë¸ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ cnn_policy.py           # CNN ì •ì±…
â”‚   â”‚   â””â”€â”€ multimodal_policy.py    # ë©€í‹°ëª¨ë‹¬ ì •ì±… (PointNet + CNN)
â”‚   â”œâ”€â”€ training/              # í•™ìŠµ ë¡œì§
â”‚   â””â”€â”€ ros2_nodes/            # ROS2 ë…¸ë“œ
â”œâ”€â”€ scripts/                   # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ extract_erp42_multiview.py  # ERP42 ë°ì´í„° ì¶”ì¶œ
â”‚   â”œâ”€â”€ preprocess_camera_only.py   # ì¹´ë©”ë¼ ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ preprocess_multimodal.py    # ë©€í‹°ëª¨ë‹¬ ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ train_camera_only.py        # ì¹´ë©”ë¼ í•™ìŠµ
â”‚   â”œâ”€â”€ train_multimodal.py         # ë©€í‹°ëª¨ë‹¬ í•™ìŠµ
â”‚   â”œâ”€â”€ compare_experiments.py      # ì‹¤í—˜ ë¹„êµ
â”‚   â””â”€â”€ run_experiment.sh           # ìë™ ì‹¤í—˜ ì‹¤í–‰
â”œâ”€â”€ configs/                   # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ experiment1_camera_only.yaml
â”‚   â””â”€â”€ experiment2_multimodal.yaml
â”œâ”€â”€ docs/                      # ë¬¸ì„œ
â”‚   â”œâ”€â”€ EXPERIMENTS.md         # ì‹¤í—˜ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ DATA_EXTRACTION.md     # ë°ì´í„° ì¶”ì¶œ ê°€ì´ë“œ
â”‚   â””â”€â”€ TRAINING.md            # í•™ìŠµ ê°€ì´ë“œ
â””â”€â”€ notebooks/                 # ë¶„ì„ ë…¸íŠ¸ë¶
\`\`\`

## ğŸ“š ë¬¸ì„œ

ìì„¸í•œ ì‚¬ìš©ë²•ì€ [Wiki](https://github.com/jeong202017/ros2-imitation-learning-ad/wiki)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ‘¥ ê¸°ì—¬

ì´ìŠˆ ë° PRì„ í™˜ì˜í•©ë‹ˆë‹¤!
