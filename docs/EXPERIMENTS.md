# ğŸ§ª Experiments / ì‹¤í—˜ ê°€ì´ë“œ

This document describes the two main experiments comparing camera-only baseline and multimodal sensor fusion approaches.

ì´ ë¬¸ì„œëŠ” ì¹´ë©”ë¼ ì „ìš© ë² ì´ìŠ¤ë¼ì¸ê³¼ ë©€í‹°ëª¨ë‹¬ ì„¼ì„œ ìœµí•© ì ‘ê·¼ ë°©ì‹ì„ ë¹„êµí•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì‹¤í—˜ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## Experiment Overview / ì‹¤í—˜ ê°œìš”

### Experiment 1: Camera-Only Baseline
**ëª©ì  (Purpose)**: Establish a baseline performance using only front camera images for control prediction.
- Single sensor modality (front camera)
- Simple CNN architecture (ResNet-18)
- Lightweight and fast inference

**ë°©ë²• (Method)**:
- Input: Front camera images from `/video1` topic
- Output: Speed (m/s) and steering (radian)
- Model: ResNet-18 based CNN encoder + MLP head
- Training: Standard behavioral cloning with MSE loss

**ì˜ˆìƒ ê²°ê³¼ (Expected Results)**:
- Faster training and inference
- Good performance in simple scenarios
- May struggle with complex driving situations or occlusions
- Limited spatial understanding

### Experiment 2: Camera + LiDAR Multimodal
**ëª©ì  (Purpose)**: Improve robustness and spatial awareness by fusing camera and LiDAR data.
- Multi-sensor fusion (camera + LiDAR)
- Enhanced spatial perception
- More robust to challenging conditions

**ë°©ë²• (Method)**:
- Input: 
  - Front camera images from `/video1`
  - LiDAR PointCloud from `/velodyne_points_filtered`
  - LaserScan from `/scan`
- Output: Speed (m/s) and steering (radian)
- Model: 
  - Camera encoder (ResNet-18)
  - LiDAR encoder (PointNet-style or 1D CNN)
  - Late fusion with concatenation
- Training: Behavioral cloning with MSE loss

**ì˜ˆìƒ ê²°ê³¼ (Expected Results)**:
- Better spatial awareness
- More robust obstacle detection
- Improved performance in complex scenarios
- Slightly slower inference due to additional processing

## Data Specifications / ë°ì´í„° ì‚¬ì–‘

### ERP42 Vehicle Control Data Format
```python
# /Control/serial_data (Float32MultiArray)
# Index 3: speed (m/s) - ì†ë„
# Index 4: steering (radian) - ì¡°í–¥ê°
control_array = [1.0, 0.0, 0.0, 5.932, -0.0708, 35.0, 0.0, 4.0]
```

### Topic Information / í† í”½ ì •ë³´
- **Camera**: `/video1` (2,404 messages) - ì „ë°© ì¹´ë©”ë¼
- **LiDAR PointCloud**: `/velodyne_points_filtered` (965 messages)
- **LaserScan**: `/scan` (965 messages)
- **Control**: `/Control/serial_data` (9,728 messages)

### Expected Dataset Size / ì˜ˆìƒ ë°ì´í„°ì…‹ í¬ê¸°
After timestamp synchronization, approximately 900-1000 samples:
- Train: ~720-800 samples (80%)
- Validation: ~90-100 samples (10%)
- Test: ~90-100 samples (10%)

## Step-by-Step Execution Guide / ì‹¤í–‰ ê°€ì´ë“œ

### Prerequisites / ì‚¬ì „ ì¤€ë¹„
```bash
# Install dependencies / ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# Verify bag file / bag íŒŒì¼ í™•ì¸
ls data/rosbags/001_0.db3
```

### Experiment 1: Camera-Only / ì‹¤í—˜ 1: ì¹´ë©”ë¼ ì „ìš©

#### Step 1: Extract Data / ë°ì´í„° ì¶”ì¶œ
```bash
python scripts/extract_erp42_multiview.py \
    --bag-path data/rosbags/001_0.db3 \
    --output-dir data/extracted/erp42_exp1 \
    --topics /video1 /Control/serial_data
```

#### Step 2: Preprocess Data / ë°ì´í„° ì „ì²˜ë¦¬
```bash
python scripts/preprocess_camera_only.py \
    --input-dir data/extracted/erp42_exp1 \
    --output-dir data/processed/experiment1 \
    --train-split 0.8 \
    --val-split 0.1 \
    --time-threshold 0.1
```

#### Step 3: Train Model / ëª¨ë¸ í•™ìŠµ
```bash
python scripts/train_camera_only.py \
    --config configs/experiment1_camera_only.yaml \
    --data-dir data/processed/experiment1 \
    --output-dir models/experiment1 \
    --epochs 100
```

#### Step 4: Monitor Training / í•™ìŠµ ëª¨ë‹ˆí„°ë§
```bash
tensorboard --logdir models/experiment1/logs
```

### Experiment 2: Multimodal / ì‹¤í—˜ 2: ë©€í‹°ëª¨ë‹¬

#### Step 1: Extract Data / ë°ì´í„° ì¶”ì¶œ
```bash
python scripts/extract_erp42_multiview.py \
    --bag-path data/rosbags/001_0.db3 \
    --output-dir data/extracted/erp42_exp2 \
    --topics /video1 /velodyne_points_filtered /scan /Control/serial_data
```

#### Step 2: Preprocess Data / ë°ì´í„° ì „ì²˜ë¦¬
```bash
python scripts/preprocess_multimodal.py \
    --input-dir data/extracted/erp42_exp2 \
    --output-dir data/processed/experiment2 \
    --train-split 0.8 \
    --val-split 0.1 \
    --time-threshold 0.1
```

#### Step 3: Train Model / ëª¨ë¸ í•™ìŠµ
```bash
python scripts/train_multimodal.py \
    --config configs/experiment2_multimodal.yaml \
    --data-dir data/processed/experiment2 \
    --output-dir models/experiment2 \
    --epochs 100
```

#### Step 4: Monitor Training / í•™ìŠµ ëª¨ë‹ˆí„°ë§
```bash
tensorboard --logdir models/experiment2/logs
```

### Compare Results / ê²°ê³¼ ë¹„êµ

```bash
python scripts/compare_experiments.py \
    --exp1-dir models/experiment1 \
    --exp2-dir models/experiment2 \
    --output-dir results/comparison
```

This will generate:
- Training curves comparison (í•™ìŠµ ê³¡ì„  ë¹„êµ)
- Evaluation metrics table (í‰ê°€ ì§€í‘œ í…Œì´ë¸”)
- Visualization plots (ì‹œê°í™” ê·¸ë˜í”„)

## Evaluation Metrics / í‰ê°€ ì§€í‘œ

Both experiments are evaluated using:

**Regression Metrics**:
- **MAE (Mean Absolute Error)**: Average absolute difference between predictions and ground truth
- **MSE (Mean Squared Error)**: Average squared difference (penalizes large errors)
- **RMSE (Root Mean Squared Error)**: Square root of MSE
- **RÂ² Score**: Coefficient of determination (closer to 1 is better)

**Per-Variable Metrics**:
- Speed MAE/MSE/RÂ²
- Steering MAE/MSE/RÂ²

## Results Interpretation / ê²°ê³¼ í•´ì„

### What to Look For / í™•ì¸ ì‚¬í•­

1. **Training Convergence / í•™ìŠµ ìˆ˜ë ´**
   - Does the loss decrease smoothly?
   - Is there overfitting (train loss << val loss)?

2. **Prediction Accuracy / ì˜ˆì¸¡ ì •í™•ë„**
   - Lower MAE/MSE is better
   - Higher RÂ² is better (closer to 1)

3. **Model Comparison / ëª¨ë¸ ë¹„êµ**
   - Does multimodal outperform camera-only?
   - What is the performance vs. complexity trade-off?

4. **Failure Cases / ì‹¤íŒ¨ ì¼€ì´ìŠ¤**
   - Where does each model struggle?
   - Are errors correlated with specific scenarios?

## Troubleshooting / ë¬¸ì œ í•´ê²°

### Common Issues / ì¼ë°˜ì ì¸ ë¬¸ì œ

**Problem**: Not enough synchronized samples
**Solution**: Increase `--time-threshold` in preprocessing

**Problem**: Training loss not decreasing
**Solution**: 
- Reduce learning rate
- Check data normalization
- Verify data loading is correct

**Problem**: Overfitting (val loss increases)
**Solution**:
- Increase dropout
- Enable data augmentation
- Reduce model complexity

**Problem**: Out of memory during training
**Solution**:
- Reduce batch size
- Use smaller images (e.g., 128x128 instead of 224x224)
- Use gradient accumulation

## Next Steps / ë‹¤ìŒ ë‹¨ê³„

After completing both experiments:

1. **Analyze Results**: Compare metrics and identify strengths/weaknesses
2. **Hyperparameter Tuning**: Experiment with different learning rates, architectures
3. **Data Augmentation**: Add more augmentation for robustness
4. **Temporal Modeling**: Try LSTM or temporal convolutions
5. **Real Vehicle Testing**: Deploy best model on ERP42 vehicle

## References / ì°¸ê³  ìë£Œ

- Original Behavioral Cloning paper: [End-to-End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316)
- Multimodal Fusion: [MultiNet](https://arxiv.org/abs/1809.08009)
- PointNet: [Deep Learning on Point Sets](https://arxiv.org/abs/1612.00593)
